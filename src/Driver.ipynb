{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4623c9-3c35-4cc6-b1ad-6c9ff470a897",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6eedd-0313-4f57-95c9-8a68c980b081",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebff2f4-5874-48de-9f5a-45870d96a2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from data_utils import DatasetUtils\n",
    "from model_utils import ModelUtils\n",
    "from result_utils import ResultUtils\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchvision.models import densenet201, inception_v3, resnet50, swin_v2_t\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcec4bb-3dff-40a3-88a9-62b866edd98b",
   "metadata": {},
   "source": [
    "## Data Set and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42ac03c-90f3-42a5-a2da-9b5259a8348f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to your dataset\n",
    "Target_addr = '/data/NeuroRG/Data/numpy/Neuroinflammation/230331/'\n",
    "Output_address = '/data/NEURORG_PAPER/rst/'\n",
    "\n",
    "# Define plate and severe lists\n",
    "plate_list = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "plate_list_split = DatasetUtils.get_splited_list(plate_list)\n",
    "severe_list = ['0uM', '0.005uM', '0.05uM', '0.5uM', '5uM', '20uM']\n",
    "\n",
    "# Get normalization parameters\n",
    "mean_list, std_list = DatasetUtils.get_normalize_parameters(Target_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3bb84-f777-461a-a837-da4c66a54370",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a2c0e7-c53b-4111-857a-a24e6f688c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'efficientnet_b5'\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize the result lists\n",
    "acc_per_img = []\n",
    "acc_per_well = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad8cffa-4b44-42f8-b29b-1c46ea72bb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, plate_idx, dataloaders, dataset_sizes, device, target_addr, model_name, num_epochs=25):\n",
    "        since = time.time()\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        PATH = f\"{target_addr}/Parameters/state_dict\"\n",
    "        os.makedirs(os.path.dirname(PATH), exist_ok=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        with torch.cuda.amp.autocast(enabled=phase == 'train'):\n",
    "                            outputs = model(inputs)\n",
    "                            if isinstance(outputs, tuple):  # Inception v3\n",
    "                                outputs = outputs[0]\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(copy.deepcopy(model.module.state_dict()), f'{PATH}_{model_name}_{plate_idx}')\n",
    "        else:\n",
    "            torch.save(copy.deepcopy(model.state_dict()), f'{PATH}_{model_name}{plate_idx}')\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9b986-5011-40b7-bebd-887bbad79775",
   "metadata": {},
   "source": [
    "# Ensemble Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06955a8-765e-4680-9dd1-343578d10e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plates(validations): ['1', '5']\n",
      "train --- Numpy data shape: (2700, 1200, 1200, 3), ---- Numpy data mix shape: (2700,)\n",
      "Counter({0: 450, 1: 450, 2: 450, 3: 450, 4: 450, 5: 450})\n",
      "val --- Numpy data shape: (1800, 1200, 1200, 3), ---- Numpy data mix shape: (1800,)\n",
      "Counter({0: 300, 1: 300, 2: 300, 3: 300, 4: 300, 5: 300})\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "4444444444\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 1.6360 Acc: 0.2967\n",
      "val Loss: 1.1037 Acc: 0.5044\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.2290 Acc: 0.4952\n",
      "val Loss: 1.2891 Acc: 0.4650\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0053 Acc: 0.5959\n",
      "val Loss: 0.8603 Acc: 0.6944\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for plate_idx, val_plate_list in enumerate(plate_list_split):\n",
    "    print(f'Plates(validations): {val_plate_list}')\n",
    "\n",
    "    # 1. Splitting the Dataset: Train vs. Validation\n",
    "    train_plate_list = list(set(plate_list) - set(val_plate_list))\n",
    "    \n",
    "    # 2. Loading the Dataset and Configuring Data Loader\n",
    "    train_dataset = DatasetUtils.get_dataset(Target_addr, 'train', train_plate_list, severe_list, mean_list, std_list, model_name)\n",
    "    val_dataset = DatasetUtils.get_dataset(Target_addr, 'val', val_plate_list, severe_list, mean_list, std_list, model_name)\n",
    "    \n",
    "    # 3. Configuring Data Loader\n",
    "    dataloaders = {\n",
    "                'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16,\n",
    "                                    pin_memory=torch.cuda.is_available()),\n",
    "                'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=16,\n",
    "                                  pin_memory=torch.cuda.is_available())\n",
    "    }\n",
    "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "    \n",
    "    # 4. Initializing and Configuring the Model\n",
    "    model_ft = ModelUtils.initialize_model(model_name, num_classes=6)\n",
    "    model_ft = torch.nn.DataParallel(model_ft, device_ids=[0, 1, 2, 3]) # Specify your device ids!!\n",
    "    model_ft = model_ft.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for idx, (n, p) in enumerate(model_ft.named_parameters()):\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    optimizer_ft = ModelUtils.get_optimizer(model_ft, model_name)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.25)\n",
    "    \n",
    "    print('4'*10)\n",
    "    \n",
    "    # 5. Training the Model\n",
    "    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, plate_idx, dataloaders, dataset_sizes, device, Output_address, model_name, num_epochs = 20)\n",
    "    \n",
    "    print('5'*10)\n",
    "    \n",
    "    # 6. Model Evaluation (with the validation set)\n",
    "    well_df = get_val_csv(Target_addr, 'val', val_plate_list, severe_list_sorted)\n",
    "    dataset_test = get_dataset(Target_addr, 'test', val_plate_list, severe_list_sorted, mean_list, std_list, model_name)\n",
    "    dataloaders_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=16,\n",
    "                                   pin_memory=torch.cuda.is_available())\n",
    "    dataset_sizes_test = len(dataloaders_test)\n",
    "    \n",
    "    model_ft.eval()\n",
    "    prediction = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in dataloaders_test:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model_ft(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                prediction.append(predicted.cpu().numpy())\n",
    "                labels_list.append(labels.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print('Accuracy of the model on the val images: {} %'.format(100 * correct / total))\n",
    "            acc_per_img.append(100 * correct / total)\n",
    "    \n",
    "    print('6'*10)\n",
    "    \n",
    "    # 7. Saving the Results\n",
    "    well_df['prediction'] = np.concatenate(prediction)\n",
    "    well_df['label'] = np.concatenate(labels_list)\n",
    "    well_df['model'] = model_name\n",
    "    well_df['day'] = day\n",
    "    well_df['module'] = module_name\n",
    "    well_df['phase'] = 'train'\n",
    "    well_df['plate_idx'] = plate_idx\n",
    "    well_df['val_plates'] = '_'.join(val_plate_list)\n",
    "    well_df.to_csv(f'{Output_address}/Val_predicted_image_{model_name}_{module_name}_{day}_val_{plate_idx}.csv')\n",
    "\n",
    "    result = well_df[['module', 'phase', 'day', 'model', 'plate_idx', 'val_plates', 'class', 'well', 'prediction', 'label']].groupby(['module', 'phase', 'day', 'model', 'plate_idx', 'val_plates', 'class', 'well']).apply(compute_ratios)\n",
    "\n",
    "    num_of_class = len(severe_list_sorted)\n",
    "    acc_prediction = sum(result['check']) / len(result) * 100\n",
    "\n",
    "    print(f'Prediction accuracy for Wells: {acc_prediction}')\n",
    "    acc_per_well.append(acc_prediction)\n",
    "\n",
    "    result.to_csv(f'{Output_address}/Val_predicted_result_{model_name}_{module_name}_{day}_val_{plate_idx}.csv')\n",
    "    \n",
    "    print('7'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557362ac-0fb1-40e2-9e2a-395fe992397a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
